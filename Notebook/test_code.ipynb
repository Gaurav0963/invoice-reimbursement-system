{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c80adc02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Company Name: IAI Solution Policy Title: Employee Reimbursement Policy Version: 1.01. Purpose The purpose of this policy is to outline the guidelines and procedures for the reimbursement of expenses incurred by employees while performing work-related duties. This policy ensures transparency and consistency in the reimbursement process. 2. Scope This policy applies to all employees of IAI Solution who incur expenses in the course of their work duties. 3. Reimbursement Categories The following categories of expenses are eligible for reimbursement under this policy: ● Food and Beverages ● Travel Expenses ● Accommodations 4. General Guidelines ● All reimbursements must be supported by original receipts and submitted within 30 days of the expense incurred. ● Employees must complete the reimbursement request form and submit it along with the required documentation to the HR department. 5. Specific Expense Guidelines 5.1 Food and Beverages ● Eligibility: Reimbursement for meals is allowed when traveling for work or attending business meetings. ● Limits: We have set food allowances for food reimbursements of ₹200 per meal. ● Restrictions: Alcoholic beverages are not reimbursable. 5.2 Travel Expenses ● Eligibility: Travel expenses are reimbursable for work-related travel only. ● Limits: We have set allowances for travel reimbursements of ₹2,000 per trip, depending on the location and the employee's level. The allowance for daily office cabs is ₹150. ● Restrictions: Any travel-related expenses incurred for personal reasons will not be reimbursed. 5.3 Accommodation ● Eligibility: Reimbursement for hotel stays is allowed for overnight business travel. ● Limits: Up to ₹50 per night, excluding taxes and fees. ● Restrictions: Employees must use company-approved hotels when available. 6. Submission Process 1. Complete the reimbursement request form. 2. Attach all relevant receipts. 3. Submit to the HR department for approval. 7. Review and Approval HR will review submissions for compliance with this policy and will either approve or deny the request within 10 business days. 8. Policy Amendments This policy may be amended at any time with prior notice to employees.\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import PyPDF2\n",
    "from groq import Groq\n",
    "import re\n",
    "import os\n",
    "import getpass\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "from io import BytesIO\n",
    "import io\n",
    "from zipfile import ZipFile\n",
    "import zipfile\n",
    "import tempfile\n",
    "import pdfplumber\n",
    "\n",
    "\n",
    "zip_file_path = \"E:\\\\FastAPI-chatbot\\\\data\\\\Cab_Bills.zip\"\n",
    "policy_pdf_path = \"E:\\\\FastAPI-chatbot\\\\data\\\\Policy-Nov-2024.pdf\"\n",
    "sample_invoice = \"E:\\\\RAGbot\\\\Book-cab-02.pdf\"\n",
    "\n",
    "load_dotenv()\n",
    "class Config:\n",
    "    GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\") or getpass.getpass(\"Groq API Key: \")\n",
    "    FAST_API_URL = os.getenv(\"API_URL\") or getpass.getpass(\"FastAPI URL: \")\n",
    "    INDEX_NAME = \"invoice-analysis\"\n",
    "    EMBEDDING_MODEL = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "    LLM_MODEL = \"llama3-70b-8192\"\n",
    "    LLM_TEMPERATURE = 0.3\n",
    "    SEARCH_CONFIG = {\"k\": 1, \"score_threshold\": 0.5}\n",
    "    VECTOR_STORE_DIR = \"./vectorDB\"\n",
    "    DB_NAME = \"invoice_analysis_report\"\n",
    "\n",
    "config = Config()\n",
    "client = Groq(api_key=config.GROQ_API_KEY)\n",
    "\n",
    "\n",
    "def extract_text_from_pdf(pdf_file_path: str) -> str:\n",
    "    text = \"\"\n",
    "    try:\n",
    "        with open(pdf_file_path, \"rb\") as file:\n",
    "            pdf_bytes_io = io.BytesIO(file.read())\n",
    "\n",
    "        with pdfplumber.open(pdf_bytes_io) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                page_text = page.extract_text()\n",
    "                if page_text:  # Ensure page_text is not None\n",
    "                    text += page_text + \"\\n\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing PDF: {e}\")\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean and normalize extracted text.\"\"\"\n",
    "    # Remove excessive whitespace and newlines\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def clean_text_2(text):\n",
    "    # Fixing broken words (e.g., \"Cust omer\" -> \"Customer\")\n",
    "    text = re.sub(r'(\\b\\w)\\s+(\\w\\b)', r'\\1\\2', text)  \n",
    "    # Normalize spaces/formatting further\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "policy_text = clean_text_2(extract_text_from_pdf(policy_pdf_path))\n",
    "policy_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f42074e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_documents_text_2(invoice_text: str, policy_text: str) -> dict:\n",
    "    \"\"\"Compare invoice with policy and get reimbursement decision with robust JSON handling.\"\"\"\n",
    "    \n",
    "    # Prepare the prompt with strict JSON formatting\n",
    "    prompt = f'''You're Insurance claims analyst. Analyze this invoice against the policy and provide a response in EXACTLY this JSON format:\n",
    "```json\n",
    "{{\n",
    "    \"customer_name\": \"Customer Name here\",\n",
    "    \"reimbursement_status\": \"accept | partially accept | reject\",\n",
    "    \"reason\": \"Detailed explanation with Specific policy clauses and Approved Amount here\",\n",
    "    \"date\": \"Invoice Date Here\",\n",
    "    \"invoice_ID\": \"Invoice ID Here\",\n",
    "    \"invoice_text\": \"specify invoice text content here\"\n",
    "}}\n",
    "\n",
    "Policy Document:\n",
    "{policy_text}\n",
    "\n",
    "Invoice Details:\n",
    "{invoice_text}\n",
    "\n",
    "Important Rules:\n",
    "1. All fields must be present\n",
    "2. Values must be in double quotes\n",
    "3. \"reimbursement_status\" must be one of: accept, partially accept, reject\n",
    "4. Do not include any text outside the JSON brackets\n",
    "5. First Name, Second Name and Thrid Name will all strat with a capital Letter.\n",
    "6. Ensure the 'reason' field in the JSON does not contain unescaped quotes or special characters.\n",
    "\n",
    "Take care of some broken words:\n",
    "1. **Cust omer Name** is **Customer Name**\n",
    "2. **Inv oice Date** is **Date**\n",
    "3. **Inv oice ID** is **Invoice ID**\n",
    "\n",
    "'''\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            model=\"llama3-70b-8192\",\n",
    "            response_format={\"type\": \"json_object\"},\n",
    "            temperature=0.1\n",
    "        )\n",
    "        \n",
    "        content = response.choices[0].message.content\n",
    "        \n",
    "        # First try to parse content directly\n",
    "        try:\n",
    "            result = json.loads(content)\n",
    "        except json.JSONDecodeError:\n",
    "            # If direct parse fails, try to extract JSON from response\n",
    "            json_match = re.search(r'\\{.*\\}', content, re.DOTALL)\n",
    "            if json_match:\n",
    "                result = json.loads(json_match.group())\n",
    "            else:\n",
    "                return {\n",
    "                    \"error\": \"No valid JSON found\",\n",
    "                    \"raw_response\": content\n",
    "                }\n",
    "        \n",
    "        # LLMs keeps missing decision key, so enforcing that \n",
    "        required_fields = [\n",
    "            \"customer_name\", \"reimbursement_status\", \"reason\", \n",
    "            \"date\", \"invoice_ID\",  \"invoice_text\"\n",
    "        ]\n",
    "        \n",
    "        for field in required_fields:\n",
    "            if field not in result:\n",
    "                raise ValueError(f\"Missing required field: {field}\")\n",
    "        \n",
    "        # Validate decision values\n",
    "        if result[\"reimbursement_status\"] not in [\"accept\", \"partially accept\", \"reject\"]:\n",
    "            raise ValueError(\"Invalid decision value\")\n",
    "            \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"error\": str(e),\n",
    "            \"raw_response\": content if 'content' in locals() else None\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2523adb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tax Invoice Driver Trip Invoice Munna ServiceTaxCategory:RentingofCab Car 2KA444 InvoiceID49434 InvoiceDate14Jun2024 CustomerNameAnjaneyaK MobileNumber8901233212 PickupAddress #12,2ndLayoutShivajiNagar Description Amount(₹) RideFee ₹282.20 TollConveniencefee ₹0 AirportCharges ₹0 CGST9.00% 25.40 SGST9.00% 25.40 Subtotal ₹333 ₹ 333 Total CustomerRide Fare\n"
     ]
    }
   ],
   "source": [
    "def clean_invoice(text):\n",
    "    \"\"\"Fix broken words and normalize spacing in extracted text.\"\"\"\n",
    "    # Fix common broken patterns (e.g., \"A njane y a K\" -> \"Anjaneya K\")\n",
    "    text = re.sub(r'(\\b[A-Za-z])\\s+([a-z]\\b)', r'\\1\\2', text)  # Fix name fragments\n",
    "    text = re.sub(r'(\\b[A-Za-z]{2})\\s+([a-z]+\\b)', r'\\1\\2', text)  # Fix words like \"Inv oice\"\n",
    "    \n",
    "    # Fix specific known issues\n",
    "    text = re.sub(r'T ax', 'Tax', text)\n",
    "    text = re.sub(r'Inv oice', 'Invoice', text)\n",
    "    text = re.sub(r'Cust omer', 'Customer', text)\n",
    "    text = re.sub(r'Addr ess', 'Address', text)\n",
    "    text = re.sub(r'Ser vice', 'Service', text)\n",
    "    text = re.sub(r'Categor y', 'Category', text)\n",
    "    text = re.sub(r'Driv er', 'Driver', text)\n",
    "    text = re.sub(r'T rip', 'Trip', text)\n",
    "    text = re.sub(r'La y out', 'Layout', text)\n",
    "    text = re.sub(r'Char ges', 'Charges', text)\n",
    "    text = re.sub(r'Conv enience', 'Convenience', text)\n",
    "    text = re.sub(r'Descri ption', 'Description', text)\n",
    "    \n",
    "    # Remove excessive whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    inv6_path = \"E:\\\\FastAPI-chatbot\\\\data\\\\Cab_Bills\\\\Cab Bills\\\\Book-cab-06.pdf\"\n",
    "    cleaned_text=clean_invoice(extract_text_from_pdf(inv6_path))\n",
    "    print(cleaned_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6ba7ab2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Extracting zip: E:\\FastAPI-chatbot\\data\\Cab_Bills.zip to C:\\Users\\Dell1\\AppData\\Local\\Temp\\tmptqha268f\n",
      "[PROGRESS] Processing file 1/10: Book-cab-01 (1).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PROGRESS] Processing file 2/10: Book-cab-02.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PROGRESS] Processing file 3/10: Book-cab-03.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PROGRESS] Processing file 4/10: Book-cab-04.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PROGRESS] Processing file 5/10: Book-cab-05.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PROGRESS] Processing file 6/10: Book-cab-06.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PROGRESS] Processing file 7/10: Book-cab-07.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PROGRESS] Processing file 8/10: Book-cab-08.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PROGRESS] Processing file 9/10: Book-cab-09.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PROGRESS] Processing file 10/10: Book-cab-10.pdf\n",
      "\n",
      "Final Results: Processed 10 files\n",
      "Sample decision: {'customer_name': 'Reetu', 'reimbursement_status': 'accept', 'reason': 'The invoice is eligible for reimbursement as per policy clause 5.2 Travel Expenses, with a limit of ₹2,000 per trip. The ride fee of ₹187.46 is within the approved limit.', 'date': '10 Sep 2024', 'invoice_ID': '397373', 'invoice_text': 'Ride Fee, Toll Convenience fee, Airport Charges'}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import zipfile\n",
    "import tempfile\n",
    "import pdfplumber\n",
    "from typing import Tuple, List\n",
    "\n",
    "# Increase recursion limit (with caution)\n",
    "sys.setrecursionlimit(10000)\n",
    "\n",
    "def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "    \"\"\"Safely extract text from PDF using pdfplumber with enhanced error handling.\"\"\"\n",
    "    text = \"\"\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                try:\n",
    "                    page_text = page.extract_text()\n",
    "                    if page_text:\n",
    "                        text += page_text + \"\\n\"\n",
    "                except Exception as page_error:\n",
    "                    print(f\"[WARNING] Page error in {pdf_path}: {str(page_error)}\")\n",
    "                    continue\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to open {pdf_path}: {str(e)}\")\n",
    "    return text.strip()\n",
    "\n",
    "def clean_text_2(text: str) -> str:\n",
    "    \"\"\"Completely rewritten text cleaning function without recursion.\"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    \n",
    "    # Step 1: Fix common broken word patterns (iterative approach)\n",
    "    common_patterns = [\n",
    "        (r'(\\b\\w)\\s+(\\w\\b)', r'\\1\\2'),  # Fix broken words\n",
    "        (r'\\bCust\\s+omer\\b', 'Customer'),\n",
    "        (r'\\bInv\\s+oice\\b', 'Invoice'),\n",
    "        (r'\\bA\\s+pproval\\b', 'Approval'),\n",
    "    ]\n",
    "    \n",
    "    for pattern, replacement in common_patterns:\n",
    "        text = re.sub(pattern, replacement, text)\n",
    "    \n",
    "    # Step 2: Normalize all whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def process_zip_and_compare(zip_file_path: str, policy_path: str) -> Tuple[List[dict], List[str]]:\n",
    "    \"\"\"Robust ZIP processing with comprehensive error handling.\"\"\"\n",
    "    # First extract policy text\n",
    "    try:\n",
    "        policy_text = clean_text_2(extract_text_from_pdf(policy_path))\n",
    "        if not policy_text:\n",
    "            raise ValueError(\"Empty policy text after extraction\")\n",
    "    except Exception as e:\n",
    "        print(f\"[CRITICAL] Policy processing failed: {str(e)}\")\n",
    "        return [], []\n",
    "\n",
    "    results = []\n",
    "    decisions = []\n",
    "    \n",
    "    with tempfile.TemporaryDirectory() as temp_dir:\n",
    "        print(f\"[INFO] Extracting zip: {zip_file_path} to {temp_dir}\")\n",
    "        \n",
    "        try:\n",
    "            # Validate ZIP file first\n",
    "            if not zipfile.is_zipfile(zip_file_path):\n",
    "                raise ValueError(\"Invalid ZIP file format\")\n",
    "            \n",
    "            with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "                zip_ref.extractall(temp_dir)\n",
    "\n",
    "            # Process files with progress tracking\n",
    "            pdf_files = [\n",
    "                os.path.join(root, filename)\n",
    "                for root, _, files in os.walk(temp_dir)\n",
    "                for filename in files\n",
    "                if filename.lower().endswith(\".pdf\")\n",
    "            ]\n",
    "            \n",
    "            if not pdf_files:\n",
    "                print(\"[WARNING] No PDF files found in ZIP archive\")\n",
    "                return [], []\n",
    "            \n",
    "            for i, pdf_path in enumerate(pdf_files, 1):\n",
    "                print(f\"[PROGRESS] Processing file {i}/{len(pdf_files)}: {os.path.basename(pdf_path)}\")\n",
    "                \n",
    "                try:\n",
    "                    # Extract text with multiple fallback attempts\n",
    "                    inv_text = extract_text_from_pdf(pdf_path)\n",
    "                    if not inv_text:\n",
    "                        raise ValueError(\"Empty text extracted from PDF\")\n",
    "                    \n",
    "                    # Clean text with safeguards\n",
    "                    inv_txt_data = clean_invoice(inv_text)\n",
    "                    \n",
    "                    # Store results\n",
    "                    results.append(inv_txt_data)\n",
    "                    \n",
    "                    # Comparison (mock implementation - replace with your actual function)\n",
    "                    decision = compare_documents_text_2(invoice_text=inv_txt_data, policy_text=policy_text)\n",
    "                    decisions.append(decision)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"[ERROR] Processing failed for {pdf_path}: {str(e)}\")\n",
    "                    results.append(f\"Error: {str(e)}\")\n",
    "                    decisions.append({\n",
    "                        \"filename\": os.path.basename(pdf_path),\n",
    "                        \"error\": str(e)\n",
    "                    })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[CRITICAL] ZIP processing failed: {str(e)}\")\n",
    "            return [], []\n",
    "\n",
    "    return decisions, results\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    zip_file_path = \"E:\\\\FastAPI-chatbot\\\\data\\\\Cab_Bills.zip\"\n",
    "    policy_pdf = \"E:\\\\FastAPI-chatbot\\\\data\\\\Policy-Nov-2024.pdf\"\n",
    "    \n",
    "    all_decisions, res = process_zip_and_compare(zip_file_path, policy_pdf)\n",
    "    print(f\"\\nFinal Results: Processed {len(all_decisions)} files\")\n",
    "    print(\"Sample decision:\", all_decisions[0] if all_decisions else \"No decisions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e748d9da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tax Invoice Driver Trip Invoice Munna ServiceTaxCategory:RentingofCab Car 2KA444 InvoiceID49434 InvoiceDate14Jun2024 CustomerNameAnjaneyaK MobileNumber8901233212 PickupAddress #12,2ndLayoutShivajiNagar Description Amount(₹) RideFee ₹282.20 TollConveniencefee ₹0 AirportCharges ₹0 CGST9.00% 25.40 SGST9.00% 25.40 Subtotal ₹333 ₹ 333 Total CustomerRide Fare\n"
     ]
    }
   ],
   "source": [
    "def extract_text_from_pdf_2(pdf_path: str) -> str:\n",
    "    \"\"\"Safely extract text from PDF using pdfplumber with enhanced error handling.\"\"\"\n",
    "    text = \"\"\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                try:\n",
    "                    page_text = page.extract_text()\n",
    "                    if page_text:\n",
    "                        text += page_text + \"\\n\"\n",
    "                except Exception as page_error:\n",
    "                    print(f\"[WARNING] Page error in {pdf_path}: {str(page_error)}\")\n",
    "                    continue\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to open {pdf_path}: {str(e)}\")\n",
    "    return text.strip()\n",
    "\n",
    "def clean_inv(text: str) -> str:\n",
    "    \"\"\"Completely rewritten text cleaning function without recursion.\"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    \n",
    "    # Step 1: Fix common broken word patterns (iterative approach)\n",
    "    common_patterns = [\n",
    "        (r'(\\b\\w)\\s+(\\w\\b)', r'\\1\\2'),  # Fix broken words\n",
    "        (r'\\bCust\\s+omer\\b', 'Customer'),\n",
    "        (r'\\bInv\\s+oice\\b', 'Invoice'),\n",
    "        (r'\\bA\\s+pproval\\b', 'Approval'),\n",
    "    ]\n",
    "    \n",
    "    for pattern, replacement in common_patterns:\n",
    "        text = re.sub(pattern, replacement, text)\n",
    "    \n",
    "    # Step 2: Normalize all whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    inv6_path = \"E:\\\\FastAPI-chatbot\\\\data\\\\Cab_Bills\\\\Cab Bills\\\\Book-cab-06.pdf\"\n",
    "    cleaned_text=clean_inv(extract_text_from_pdf_2(inv6_path))\n",
    "    print(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "52e6d9b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WEST HOLLYWOOD 7677 state Los Angeles 1800 000000 Receipt No.: 3575 Table No.: 48 Date: Dec 13, 2024 18:26 Customer Name: Avinash A QTY/ Item Name Price Amount 2 Idli 25.00 50.00 1 Vada 20.00 20.00 1 Tea 10.00 10.00 Sub Total: 80.00 CGST: 5% 4.00 SGST: 5% 4.00 Total: 88.00 Payment Mode: Online PLEASE VISIT US AGAIN THANK YOU!!\n"
     ]
    }
   ],
   "source": [
    "def clean_invoice(text):\n",
    "    \"\"\"Fix broken words and normalize spacing in extracted text.\"\"\"\n",
    "    # Fix common broken patterns (e.g., \"A njane y a K\" -> \"Anjaneya K\")\n",
    "    text = re.sub(r'(\\b[A-Za-z])\\s+([a-z]\\b)', r'\\1\\2', text)  # Fix name fragments\n",
    "    text = re.sub(r'(\\b[A-Za-z]{2})\\s+([a-z]+\\b)', r'\\1\\2', text)  # Fix words like \"Inv oice\"\n",
    "    \n",
    "    # Fix specific known issues\n",
    "    text = re.sub(r'T ax', 'Tax', text)\n",
    "    text = re.sub(r'Inv oice', 'Invoice', text)\n",
    "    text = re.sub(r'Cust omer', 'Customer', text)\n",
    "    text = re.sub(r'Addr ess', 'Address', text)\n",
    "    text = re.sub(r'Ser vice', 'Service', text)\n",
    "    text = re.sub(r'Categor y', 'Category', text)\n",
    "    text = re.sub(r'Driv er', 'Driver', text)\n",
    "    text = re.sub(r'T rip', 'Trip', text)\n",
    "    text = re.sub(r'La y out', 'Layout', text)\n",
    "    text = re.sub(r'Char ges', 'Charges', text)\n",
    "    text = re.sub(r'Conv enience', 'Convenience', text)\n",
    "    text = re.sub(r'Descri ption', 'Description', text)\n",
    "    \n",
    "    # Remove excessive whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    inv6_path = \"E:\\\\FastAPI-chatbot\\\\data\\\\Cab_Bills\\\\Cab Bills\\\\Book-cab-06.pdf\"\n",
    "    path = \"E:\\\\FastAPI-chatbot\\\\data\\Meal Invoice 4.pdf\"\n",
    "    cleaned_text=clean_invoice(extract_text_from_pdf_2(path))\n",
    "    print(cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76ae677",
   "metadata": {},
   "source": [
    "# Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9beb6171",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from typing import List\n",
    "\n",
    "def get_data_to_embed(decisions: List[dict], invoice_texts: List[str]) -> List[Document]:\n",
    "    documents = []\n",
    "    \n",
    "    if len(decisions) != len(invoice_texts):\n",
    "        raise ValueError(\"Decisions and invoice texts must be of equal length\")\n",
    "    \n",
    "    for decision, invoice_text in zip(decisions, invoice_texts):\n",
    "        try:\n",
    "            # Extract core fields with defaults\n",
    "            reason = decision.get(\"reason\", \"No reason provided\")\n",
    "            status = decision.get(\"reimbursement_status\", \"unknown\").lower()\n",
    "            employee_name = decision.get(\"customer_name\", \"Unknown\")\n",
    "            \n",
    "            # Prepare document content\n",
    "            text_to_embed = f\"Invoice Content: {invoice_text}, Status: {status}, Reason: {reason}\"\n",
    "            \n",
    "            # Prepare metadata\n",
    "            metadata = {\n",
    "                \"invoice_id\": decision.get(\"invoice_ID\", \"unknown\"),\n",
    "                \"status\": status,\n",
    "                \"reason\": reason,\n",
    "                \"employee_name\": employee_name,\n",
    "                \"date\": decision.get(\"date\", \"Unknown\")\n",
    "            }\n",
    "            \n",
    "            # Creating Langchain Document\n",
    "            documents.append(Document(\n",
    "                page_content=text_to_embed,\n",
    "                metadata=metadata\n",
    "            ))\n",
    "            \n",
    "        except Exception as e:\n",
    "            continue\n",
    "            \n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7789b08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from typing import List\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "\n",
    "config = Config()\n",
    "\n",
    "class VectorStore:\n",
    "    def __init__(self, db_path: str = config.VECTOR_STORE_DIR) -> None:\n",
    "        self.embeddings = HuggingFaceEmbeddings(model_name=config.EMBEDDING_MODEL)\n",
    "        self.vector_store = Chroma(\n",
    "            collection_name=config.DB_NAME,\n",
    "            embedding_function=self.embeddings,\n",
    "            persist_directory=db_path\n",
    "        )\n",
    "\n",
    "    def add_documents(self, documents: List[Document]) -> None:\n",
    "        \"\"\"Add documents with metadata to the vector store\"\"\"\n",
    "        self.vector_store.add_documents(documents)\n",
    "        \n",
    "    def similarity_search(self, query: str, k: int = 4) -> List[Document]:\n",
    "        \"\"\"Search for similar documents\"\"\"\n",
    "        return self.vector_store.similarity_search(query, k=k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "87590f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = VectorStore()\n",
    "docs = get_data_to_embed(all_decisions, res)\n",
    "vector_store.add_documents(documents=docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "da6158d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "LANGSMITH_API_KEY = os.getenv(\"LANGSMITH_API_KEY\") or getpass.getpass(\"LANGSMITH_API_KEY: \")\n",
    "# os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bff5529",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, TypedDict\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "llm = init_chat_model(\"llama3-8b-8192\", model_provider=\"groq\")\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "\n",
    "# Define state for application\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "\n",
    "\n",
    "# Define application steps\n",
    "def retrieve(state: State):\n",
    "    retrieved_docs = vector_store.similarity_search(state[\"question\"])\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "\n",
    "def generate(state: State):\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "\n",
    "# Compile application and test\n",
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph = graph_builder.compile()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8921303f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No, Rekha is a customer name in the context.\n"
     ]
    }
   ],
   "source": [
    "response = graph.invoke({\"question\": \"Is Rekha is any customer name\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbab5eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "\n",
    "@tool(response_format=\"content_and_artifact\")\n",
    "def retrieve(query: str):\n",
    "    \"\"\"Retrieve information related to a query.\"\"\"\n",
    "    retrieved_docs = vector_store.similarity_search(query, k=2)\n",
    "    serialized = \"\\n\\n\".join(\n",
    "        (f\"Source: {doc.metadata}\\n\" f\"Content: {doc.page_content}\")\n",
    "        for doc in retrieved_docs\n",
    "    )\n",
    "    return serialized, retrieved_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90406c26",
   "metadata": {},
   "source": [
    "# RAG application using a sequence of messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ace9c4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import MessagesState, StateGraph\n",
    "\n",
    "graph_builder = StateGraph(MessagesState)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8801e402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turning the retriver into a tool\n",
    "\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "\n",
    "@tool(response_format=\"content_and_artifact\")\n",
    "def retrieve(query: str):\n",
    "    \"\"\"Retrieve information related to a query.\"\"\"\n",
    "    retrieved_docs = vector_store.similarity_search(query, k=2)\n",
    "    serialized = \"\\n\\n\".join(\n",
    "        (f\"Source: {doc.metadata}\\n\" f\"Content: {doc.page_content}\")\n",
    "        for doc in retrieved_docs\n",
    "    )\n",
    "    return serialized, retrieved_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490836d8",
   "metadata": {},
   "source": [
    "Our graph will consist of three nodes:\n",
    "\n",
    "- A node that fields the user input, either generating a query for the retriever or responding directly;\n",
    "- A node for the retriever tool that executes the retrieval step;\n",
    "- A node that generates the final response using the retrieved context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5fc477a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "\n",
    "# Step 1: Generate an AIMessage that may include a tool-call to be sent.\n",
    "def query_or_respond(state: MessagesState):\n",
    "    \"\"\"Generate tool call for retrieval or respond.\"\"\"\n",
    "    llm_with_tools = llm.bind_tools([retrieve])\n",
    "    response = llm_with_tools.invoke(state[\"messages\"])\n",
    "    # MessagesState appends messages to state instead of overwriting\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "# Step 2: Execute the retrieval.\n",
    "tools = ToolNode([retrieve])\n",
    "\n",
    "\n",
    "# Step 3: Generate a response using the retrieved content.\n",
    "def generate(state: MessagesState):\n",
    "    \"\"\"Generate answer.\"\"\"\n",
    "    # Get generated ToolMessages\n",
    "    recent_tool_messages = []\n",
    "    for message in reversed(state[\"messages\"]):\n",
    "        if message.type == \"tool\":\n",
    "            recent_tool_messages.append(message)\n",
    "        else:\n",
    "            break\n",
    "    tool_messages = recent_tool_messages[::-1]\n",
    "\n",
    "    # Format into prompt\n",
    "    docs_content = \"\\n\\n\".join(doc.content for doc in tool_messages)\n",
    "    system_message_content = (\n",
    "        \"You are an assistant for question-answering tasks. \"\n",
    "        \"Use the following pieces of retrieved context to answer \"\n",
    "        \"the question. If you don't know the answer, say that you \"\n",
    "        \"don't know. Use three sentences maximum and keep the \"\n",
    "        \"answer concise.\"\n",
    "        \"\\n\\n\"\n",
    "        f\"{docs_content}\"\n",
    "    )\n",
    "    conversation_messages = [\n",
    "        message\n",
    "        for message in state[\"messages\"]\n",
    "        if message.type in (\"human\", \"system\")\n",
    "        or (message.type == \"ai\" and not message.tool_calls)\n",
    "    ]\n",
    "    prompt = [SystemMessage(system_message_content)] + conversation_messages\n",
    "\n",
    "    # Run\n",
    "    response = llm.invoke(prompt)\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b0deb929",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "graph_builder.add_node(query_or_respond)\n",
    "graph_builder.add_node(tools)\n",
    "graph_builder.add_node(generate)\n",
    "\n",
    "graph_builder.set_entry_point(\"query_or_respond\")\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"query_or_respond\",\n",
    "    tools_condition,\n",
    "    {END: END, \"tools\": \"tools\"},\n",
    ")\n",
    "graph_builder.add_edge(\"tools\", \"generate\")\n",
    "graph_builder.add_edge(\"generate\", END)\n",
    "\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e565693e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hello, is Reema is a Customer name\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  retrieve (7q0cy0yz3)\n",
      " Call ID: 7q0cy0yz3\n",
      "  Args:\n",
      "    query: Reema\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: retrieve\n",
      "\n",
      "Source: {'employee_name': 'Rani', 'reason': 'The invoice is eligible for reimbursement as per policy clause 5.2 Travel Expenses, with a limit of ₹2,000 per trip. The amount claimed ₹150 is within the approved limit. The invoice is supported by an original receipt and submitted within 30 days of the expense incurred.', 'date': '18 Jun 2024', 'invoice_id': 'InvoiceID123335', 'status': 'accept'}\n",
      "Content: Invoice Content: Original Tax Invoice Driver Trip Invoice Rohit ServiceTaxCategory:RentingofCab Cab 4KA1233 InvoiceID123335 InvoiceDate18Jun2024 CustomerNameRani MobileNumber8901233212 PickupAddress #12,3rd LayoutRamamurthyNagar Description Amount(₹) RideFee ₹127.12 TollConveniencefee ₹0 AirportCharges ₹0 CGST9.00% 11.44 SGST9.00% 11.44 Subtotal ₹150 ₹150 Total CustomerRide Fare, Status: accept, Reason: The invoice is eligible for reimbursement as per policy clause 5.2 Travel Expenses, with a limit of ₹2,000 per trip. The amount claimed ₹150 is within the approved limit. The invoice is supported by an original receipt and submitted within 30 days of the expense incurred.\n",
      "\n",
      "Source: {'date': '14 Jun 2024', 'reason': \"The invoice is eligible for reimbursement as it falls under the 'Travel Expenses' category with a limit of ₹2,000 per trip. The ride fare of ₹111 is within the approved limit.\", 'invoice_id': '1234', 'status': 'accept', 'employee_name': 'Sunil'}\n",
      "Content: Invoice Content: Original Tax Invoice Driver Trip Invoice RahimKhan ServiceTaxCategory:RentingofCab Bike 4BJ444 InvoiceID1234 InvoiceDate14Jun2024 CustomerNameSunil MobileNumber8901233212 PickupAddress #12,2ndLayoutShivajiNagar Description Amount(₹) RideFee ₹111 TollConveniencefee ₹0 AirportCharges ₹0 CGST9.00% 0 SGST9.00% 0 Subtotal ₹111 ₹ 111 Total CustomerRide Fare, Status: accept, Reason: The invoice is eligible for reimbursement as it falls under the 'Travel Expenses' category with a limit of ₹2,000 per trip. The ride fare of ₹111 is within the approved limit.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I don't know.\n"
     ]
    }
   ],
   "source": [
    "input_message = \"Hello, is Reema is a Customer name\"\n",
    "\n",
    "for step in graph.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": input_message}]},\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    step[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e69232",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IRSbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
